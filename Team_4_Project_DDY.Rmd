---
title: "Team_4_Project_DDY"
author: "Dingyi Duan"
date: "6/7/2022"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load the data

```{r}

library(caret)
# load the dataset
OnlineNewsPopularity <- read.csv("https://raw.githubusercontent.com/dingyiduan7/ADS-503-Applied-Predictive-Modeling/main/Datasets/OnlineNewsPopularity.csv")
dim(OnlineNewsPopularity)
summary(OnlineNewsPopularity)

```
```{r}

# url doesn't contribute to predictions

df <- OnlineNewsPopularity[, -1]

X <- df[ , -which(names(df) %in% c("shares"))]
y <- df$shares


```

# EDA

```{r}

# check for missing values
col_na <-sort(colSums(is.na(X)))
barplot(sort(col_na, decreasing = TRUE), las=2, cex.names = 0.55, main = "NA by predictors")

```
No missing values. No need for imputation.

```{r}

# check for degenerate features using zero variance
degen.cols <- nearZeroVar(X)

# plot the degenerate columns
for (i in degen.cols){
  
  barplot(table(X[, c(i)]),
          main = paste("Distribution of", colnames(X)[i]),
          xlab=colnames(X)[i],
          ylab="Frequency")
}

```
One predictors is degenerate predictor: kw_min_max.

```{r}

# use barplot to check for frequency distributions for the categorical predictors
for (i in 1:ncol(df)){
  
  barplot(table(df[, c(i)]),
          main = paste("Distribution of", colnames(df)[i]),
          xlab=colnames(df)[i],
          ylab="Frequency")
}

```
Many Predictors have skewness, need to perform required transformation before training.

# Preprocessing
```{r}

# remove zero variance predictors
X <- X[, -degen.cols]

```

```{r}

# separate binary predictors and continuous predictors
bi_var <- Filter(function(x) all(x %in% c(0, 1)), X)

con_var <- X[, !names(X) %in% names(bi_var)]

```

```{r}

# calculate colinearity between binary variables
corr_bi <- cor(bi_var)
library(corrplot)
corrplot(corr_bi, order = "hclust")

# find highly correlated predictors, using cutoff value of 0.70
highCorr_bi <- findCorrelation(corr_bi, cutoff = .7)

```

```{r}

# remove highly correlated binary variables
filtered_bi <- bi_var[, -highCorr_bi] # remove highly correlated predictors
corr_bi2 <- cor(filtered_bi)
corrplot(corr_bi2, order = "hclust")

highCorr_bi2 <- findCorrelation(cor(filtered_bi), cutoff = .70) # check for highly correlated predictors again

```

```{r}

# calculate colinearity between continuous variables
corr_con <- cor(con_var)
corrplot(corr_con, order = "hclust")

# find highly correlated predictors, using cutoff value of 0.70
highCorr_con <- findCorrelation(corr_con, cutoff = .7)

```

```{r}

# remove highly correlated continuous variables
filtered_con <- con_var[, -highCorr_con] # remove highly correlated predictors
corr_con2 <- cor(filtered_con)
corrplot(corr_con2, order = "hclust")

highCorr_con2 <- findCorrelation(cor(filtered_con), cutoff = .70) # check for highly correlated predictors again

```
```{r}

# merge binary and continuous variables back together 

df <- data.frame(filtered_bi, filtered_con, y)

set.seed(100) # repeat the results

# split the train and test to 0.8/0.2
trainingRows <- createDataPartition(df$y, p = .80,  list = FALSE)

train <- df[trainingRows, ]
test <- df[-trainingRows, ]

train_X <- train[,-46]
train_y <- train$y

test_X <- test[,-46]
test_y <- test$y

```
